# Data Quality Report: –ø—Ä–æ–ø—É—Å–∫–∏ (DA-2-24), –≤—ã–±—Ä–æ—Å—ã IQR/Z (DA-1-18), –∞–Ω–æ–º–∞–ª–∏–∏ IsolationForest (DA-2-11)
# –ó–∞–ø—É—Å–∫:
#   pip install -r requirements.txt
#   python main.py            # —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ

import os
import argparse
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

OUTPUT_DIR = "dq_report"
plt.rcParams["figure.figsize"] = (8, 4)


def ensure_output_dir(path: str = OUTPUT_DIR) -> None:
    """–°–æ–∑–¥–∞—ë—Ç –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
    os.makedirs(path, exist_ok=True)

def generate_synthetic_df(n: int = 500, seed: int = 42) -> pd.DataFrame:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏, –≤—ã–±—Ä–æ—Å–∞–º–∏ –∏ –∞–Ω–æ–º–∞–ª–∏—è–º–∏."""
    rng = np.random.default_rng(seed)
    df = pd.DataFrame({
        "age": rng.normal(35, 10, n).round(0),
        "income": rng.normal(60_000, 15_000, n),
        "transactions": rng.poisson(15, n).astype(float),
        "score": rng.normal(0.6, 0.15, n),
        "category": rng.choice(["A", "B", "C"], size=n, p=[0.5, 0.3, 0.2]),
    })
    # –ø—Ä–æ–ø—É—Å–∫–∏
    for col in ["age", "income", "transactions", "score"]:
        miss_idx = rng.choice(df.index, size=int(0.1 * n), replace=False)
        df.loc[miss_idx, col] = np.nan
    miss_idx_c = df[df["category"] == "C"].sample(frac=0.25, random_state=0).index
    df.loc[miss_idx_c, "score"] = np.nan
    # –≤—ã–±—Ä–æ—Å—ã
    out_idx = rng.choice(df.index, size=10, replace=False)
    df.loc[out_idx, "income"] *= 4
    df.loc[out_idx[:5], "transactions"] += 80
    # –∞–Ω–æ–º–∞–ª–∏–∏
    anom_idx = rng.choice(df.index, size=8, replace=False)
    df.loc[anom_idx, "age"] = rng.normal(19, 2, len(anom_idx)).round(0)
    df.loc[anom_idx, "income"] = rng.normal(18_000, 3_000, len(anom_idx))
    df.loc[anom_idx, "score"] = rng.normal(0.95, 0.02, len(anom_idx))
    return df


def plot_missingness_heatmap(df: pd.DataFrame, out_dir: str = OUTPUT_DIR) -> str:
    """–°—Ç—Ä–æ–∏—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–æ–ø—É—Å–∫–æ–≤."""
    fig, ax = plt.subplots()
    ax.imshow(df.isna().astype(int).values, aspect="auto", interpolation="nearest")
    ax.set_title("Missingness Map (1 = missing)")
    ax.set_xlabel("Columns"); ax.set_ylabel("Rows")
    ax.set_yticks([])
    ax.set_xticks(range(len(df.columns)))
    ax.set_xticklabels(df.columns, rotation=45, ha="right")
    fig.tight_layout()
    path = os.path.join(out_dir, "missingness_heatmap.png")
    fig.savefig(path, dpi=160)
    plt.close(fig)
    return path


def iqr_outliers(series: pd.Series) -> Tuple[pd.Series, Tuple[float, float]]:
    """–í—ã—è–≤–ª—è–µ—Ç –≤—ã–±—Ä–æ—Å—ã –º–µ—Ç–æ–¥–æ–º –º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–∞—Ö–∞ (IQR)"""
    q1, q3 = series.quantile([0.25, 0.75])
    iqr = q3 - q1
    low, high = q1 - 1.5 * iqr, q3 + 1.5 * iqr # –≤—Å–µ, —á—Ç–æ –Ω–∏–∂–µ low/ –≤—ã—à–µ high –Ω–æ–º–∞–ª—å–Ω–æ –Ω–∏–∑–∫–æ–µ/–≤—ã—Å–æ–∫–æ–µ
    mask = (series < low) | (series > high)
    return mask, (float(low), float(high))


def zscore_outliers(series: pd.Series, thresh: float = 3.0) -> Tuple[pd.Series, Tuple[float, float]]:
    """–í—ã—è–≤–ª—è–µ—Ç –≤—ã–±—Ä–æ—Å—ã –ø–æ Z-score (|z| > –ø–æ—Ä–æ–≥–∞)"""
    mu, sd = series.mean(), series.std(ddof=0)
    if sd == 0 or np.isnan(sd):
        return pd.Series(False, index=series.index), (float(mu), float(sd))
    z = (series - mu) / sd
    return (z.abs() > thresh), (float(mu), float(sd))


def compute_outliers(df: pd.DataFrame) -> Tuple[Dict[str, int], Dict[str, int], Dict[str, Dict[str, float]]]:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—ã–±—Ä–æ—Å—ã –ø–æ IQR –∏ Z-score –¥–ª—è –≤—Å–µ—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫."""
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    iqr_counts, z_counts, bounds = {}, {}, {}
    for col in numeric_cols:
        s = df[col].dropna()
        if s.empty:
            iqr_counts[col] = 0
            z_counts[col] = 0
            bounds[col] = {"iqr_low": np.nan, "iqr_high": np.nan, "mean": np.nan, "std": np.nan}
            continue
        m_iqr, (low, high) = iqr_outliers(s)
        m_z, (mu, sd) = zscore_outliers(s)
        iqr_counts[col] = int(m_iqr.sum())
        z_counts[col] = int(m_z.sum())
        bounds[col] = {"iqr_low": low, "iqr_high": high, "mean": mu, "std": sd}
    return iqr_counts, z_counts, bounds


def detect_anomalies_isolation_forest(
    df: pd.DataFrame, numeric_cols: List[str] | None = None, seed: int = 42
) -> Tuple[np.ndarray, int, str]:
    """–ù–∞—Ö–æ–¥–∏—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –º–µ—Ç–æ–¥–æ–º IsolationForest –∏ —Å—Ç—Ä–æ–∏—Ç scatter-–≥—Ä–∞—Ñ–∏–∫."""
    if numeric_cols is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    X = df[numeric_cols].copy().fillna(df[numeric_cols].median())
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    iso = IsolationForest(random_state=seed, contamination="auto")
    pred = iso.fit_predict(X_scaled)
    anomaly_mask = pred == -1
    count = int(anomaly_mask.sum())

    x1, x2 = ("income" if "income" in X.columns else None,
              "score" if "score" in X.columns else None)
    if x1 is None or x2 is None:
        cols = X.columns.tolist()
        x1, x2 = cols[0], cols[1] if len(cols) > 1 else cols[0]

    fig, ax = plt.subplots()
    ax.scatter(X[x1], X[x2], s=10, alpha=0.6, label="normal")
    ax.scatter(X.loc[anomaly_mask, x1], X.loc[anomaly_mask, x2], s=25, marker="x", label="anomaly")
    ax.set_xlabel(x1); ax.set_ylabel(x2); ax.set_title("IsolationForest anomalies")
    ax.legend()
    path = os.path.join(OUTPUT_DIR, "isoforest_scatter.png")
    fig.tight_layout(); fig.savefig(path, dpi=160)
    plt.close(fig)
    return anomaly_mask, count, path


def build_summary_table(
    df: pd.DataFrame,
    iqr_counts: Dict[str, int],
    z_counts: Dict[str, int],
    bounds: Dict[str, Dict[str, float]],
) -> pd.DataFrame:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏, –≤—ã–±—Ä–æ—Å–∞–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π."""
    missing_counts = df.isna().sum()
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    rows = []
    for col in df.columns:
        r = {"column": col, "dtype": str(df[col].dtype), "missing": int(missing_counts[col])}
        if col in numeric_cols:
            r.update({
                "iqr_outliers": iqr_counts.get(col, 0),
                "z_outliers": z_counts.get(col, 0),
                **bounds.get(col, {"iqr_low": np.nan, "iqr_high": np.nan, "mean": np.nan, "std": np.nan}),
            })
        else:
            r.update({"iqr_outliers": None, "z_outliers": None,
                      "iqr_low": None, "iqr_high": None, "mean": None, "std": None})
        rows.append(r)
    return pd.DataFrame(rows)


def save_markdown_report(
    heatmap_path: str,
    summary_csv_path: str,
    anomaly_count: int,
    missing_cols_with_gaps: int,
    num_iqr_cols: int,
    num_z_cols: int,
    out_dir: str = OUTPUT_DIR,
) -> str:
    path = os.path.join(out_dir, "data_quality_report.md")
    with open(path, "w", encoding="utf-8") as f:
        f.write(f"""# –û—Ç—á—ë—Ç –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö

## 1. –ö–∞—Ä—Ç–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
–§–∞–π–ª: `{heatmap_path}`

## 2. –í—ã–±—Ä–æ—Å—ã
- IQR: Q1-1.5*IQR, Q3+1.5*IQR
- Z-score: |z| > 3

## 3. –ê–Ω–æ–º–∞–ª–∏–∏ (IsolationForest)
- –ú–µ–¥–∏–∞–Ω–Ω–∞—è –∏–º–ø—É—Ç–∞—Ü–∏—è + —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
- –ù–∞–π–¥–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: **{anomaly_count}**

## 4. –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
CSV: `{summary_csv_path}`

## 5. –ò—Ç–æ–≥
- –ö–æ–ª–æ–Ω–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏: **{missing_cols_with_gaps}**
- –ö–æ–ª–æ–Ω–æ–∫ —Å IQR-–≤—ã–±—Ä–æ—Å–∞–º–∏: **{num_iqr_cols}**
- –ö–æ–ª–æ–Ω–æ–∫ —Å Z-–≤—ã–±—Ä–æ—Å–∞–º–∏: **{num_z_cols}**

Baseline –≤—ã–ø–æ–ª–Ω–µ–Ω (–æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ ‚â•3 —Ç–∏–ø–æ–≤ –ø—Ä–æ–±–ª–µ–º).
""")
    return path

def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π –∞–Ω–∞–ª–∏–∑–∞: –∑–∞–≥—Ä—É–∑–∫–∞, —Ä–∞—Å—á—ë—Ç, –æ—Ç—á—ë—Ç."""
    ensure_output_dir()

    # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = generate_synthetic_df()

    synthetic_csv_path = os.path.join(OUTPUT_DIR, "synthetic_data.csv")
    df.to_csv(synthetic_csv_path, index=False, encoding="utf-8")
    print(f"üìÑ –°–æ—Ö—Ä–∞–Ω—ë–Ω —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç: {synthetic_csv_path}")
    print(df.head(), "\n")  # –ø–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏

    # 2. –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    heatmap_path = plot_missingness_heatmap(df)
    iqr_counts, z_counts, bounds = compute_outliers(df)
    _, anomaly_count, anom_plot_path = detect_anomalies_isolation_forest(df)

    # 3. –§–æ—Ä–º–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É
    summary_df = build_summary_table(df, iqr_counts, z_counts, bounds)
    summary_csv_path = os.path.join(OUTPUT_DIR, "data_quality_summary.csv")
    summary_df.to_csv(summary_csv_path, index=False, encoding="utf-8")

    # 4. Markdown-–æ—Ç—á—ë—Ç
    missing_cols_with_gaps = int((df.isna().sum() > 0).sum())
    num_iqr_cols = sum(v > 0 for v in iqr_counts.values())
    num_z_cols = sum(v > 0 for v in z_counts.values())
    report_path = save_markdown_report(
        heatmap_path, summary_csv_path, anomaly_count,
        missing_cols_with_gaps, num_iqr_cols, num_z_cols,
    )

    # 5. –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    print("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω")
    print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {OUTPUT_DIR}/")
    print(f"–î–∞–Ω–Ω—ã–µ:     {synthetic_csv_path}")
    print(f"Heatmap:   {heatmap_path}")
    print(f"Anomalies: {anom_plot_path}")
    print(f"Summary:   {summary_csv_path}")
    print(f"Report:    {report_path}")


if __name__ == "__main__":
    main()
